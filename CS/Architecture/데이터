### 비트와 바이트

컴퓨터는 0과 1만 이해할 수 있다. 여기서 0과 1은 '정보단위' 라고 하는데, 영어로는 '비트(bit)'라고 한다.
컴퓨터가 숫자나 문자와같은 데이터를 표현하는 방법을 이해하기 위해서는 우선 정보단위인 '비트(bit)' 에 대해서 먼저 알아야 한다.

우리가 사용하는 프로그램은 수많은 비트들로 이루어져 있다. 하지만 프로그램을 비트단위로 표현하지는 않는다. 
비트단위로 표현하게 되면 숫자가 너무 커져버리기 때문인데, 그래서 우리는 비트보다 큰 단위인 '바이트(byte)'를 사용하여 의사소통한다.

- 단위 -
1 바이트(byte) == 8 비트(bit)
1 킬로바이트(1KB) == 1000 바이트
1 메가바이트(1MB) == 1000 킬로바이트
1 기가바이트(1GB) == 1000 메가바이트
1 테라바이트(1TB) == 1000 기가바이트

(이전 단위를 1024개씩 묶은 단위는 1KiB, 1MiB, 1GiB ... 라고 표현한다)

-----------------------------------------------------------------------------------------------------------------------
* 워드(Word) : CPU가 한 번에 처리할 수 있는 정보단위
              (예를들어, CPU가 한 번에 32비트씩 처리할 수 있는 CPU이다? 그 CPU 환경에서는 32비트가 1워드)
  
  - 하프워드 : 워드의 절반 크기
  - 풀 워드  : 워드 크기
  - 더블워드 : 워드의 두배 크기

-----------------------------------------------------------------------------------------------------------------------
* 코드상에서 이진법과 십육진법 표현 방법 : 이진법 0b / 십육진법 0x

이진법과 십육진법의 변환이 매우쉽다.
십육진수 -> 이진수 : 십육진수의 한자리를 이진수로 표현해서 이어붙이면 끝.
이진수 -> 십육진수 : 이진수의 네자리마다 끊어서 십육진수로 바꾸고 붙이면 끝.

십육진법이 왜 나왔나? 길어지는 숫자를 짧고 간결하게 표현하기 위해 나왔다.

-----------------------------------------------------------------------------------------------------------------------
* 이진법으로 음수 표현하기

컴퓨터는 0과 1을 제외한 모든것을 모르기에, 역시나 음수도 모른다.
그럼 컴퓨터가 음수를 이해할 수 있도록 하는 방법은 뭐가 있을까? 바로 '2의 보수'를 사용하는 것이다.
2의 보수란 말이 어려운데, '어떤 수를 그보다 큰 2^n 에서 뺀 값' 이다.

더 어렵다.
그래서 가장 쉽게 이해하는 방법은, '모든 0과 1을 뒤집고 1을 더한값' 이 바로 2의 보수이다.
11 -> 00 -> 01

매우 쉽다. 어렵게 2의 보수의 정의를 생각하지 말고, '모든 0과 1을 뒤집고 1을 더한값' 이라고 쉽게 생각하고 사용하자.  

근데, 여기까지 생각해 봤을 때, 양수 101과, 음수 101을 어떻게 구분하는지 의문이 들 것이다.
그래서 CPU 내부에는 이를 구분하기 위한 '플래그(flag)' 라는 것이 있다. 
플래그는 플래그 레지스터에 표시되는 값인데, 컴퓨터는 그 플래그를 보고 양수 플래그면 양수, 음수 플래그면 음수로 판단한다.
즉, 컴퓨터를 이해시키기 위해 모든 숫자는 플래그를 달고 다닌다고 생각하면 된다.
(CPU 내부에는 '플래그 레지스터' 라는 특별한 레지스터가 있는데 이건 나중에 CPU 공부할 때 자세히 알아보자)

-----------------------------------------------------------------------------------------------------------------------
* 문자표현

컴퓨터는 문자집합 내에 속한 문자들만 이해 가능.

- 인코딩(encoding) : 인간의 언어를 컴퓨터가 이해하는 언어(0과 1)로 변환하는 과정
- 디코딩(decoding) : 컴퓨터가 이해하는 언어(0과 1)을 인간의 언어로 변환하는 과정
